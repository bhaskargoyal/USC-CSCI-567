\documentclass[a4paper, 12pt]{article}

\newcommand{\assignmentAuthor}{Bhaskar Goyal}
\newcommand{\course}{CSCI - 567}
\newcommand{\assignmentName}{HW {04}}
\newcommand{\USCID}{6547367383}
\newcommand{\assignmentDate}{{Aug 3rd, 2021}}

\addtolength{\hoffset}{-2.25cm}
\addtolength{\textwidth}{4.5cm}
\addtolength{\voffset}{-2.5cm}
\addtolength{\textheight}{5cm}
\setlength{\parskip}{0pt}
\setlength{\parindent}{15pt}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{automata}
\usepackage{enumitem}
\usepackage{mathabx}

\usepackage{graphicx}
\usepackage{multicol}
\usepackage{ marvosym }
\usepackage{wasysym}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usepackage{fancyhdr}
\usepackage{amssymb,latexsym,amsmath,amsthm}
\usepackage{amsfonts,rawfonts}
\usepackage{thmtools}
\usepackage{systeme}
\usepackage{mathtools}
\usepackage{algorithmic}
\usepackage[colorlinks = true, linkcolor = black, citecolor = black, final]{hyperref}
\usepackage{tikz}
\usetikzlibrary{automata}

\usepackage{graphicx}
\usepackage{multicol}
\usepackage{ marvosym }
\usepackage{wasysym}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usepackage{fancyhdr}
\usepackage{amssymb,latexsym,amsmath,amsthm}
\usepackage{amsfonts,rawfonts}
\usepackage{thmtools}
\usepackage{systeme}
\usepackage{mathtools}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\max}{max}
\DeclareMathOperator*{\min}{min}

\pagestyle{fancy}
\fancyhf{}
\rhead{\assignmentAuthor \; (USC ID - \USCID)}
\lhead{\course \; \assignmentName}
\rfoot{Page \thepage}
\lfoot{\assignmentDate}

\newcommand{\ds}{\displaystyle}

\setlength{\parindent}{0in}

\declaretheoremstyle[
headfont=\color{blue}\normalfont\bfseries,
notefont=\bfseries, 
notebraces={}{},
% bodyfont=\color{red}\normalfont\itshape,
bodyfont=\normalfont,%\itshape,
%headformat=\NUMBER.~\NAME\NOTE
headformat=\NAME\NOTE
]{colorquestion}

\declaretheorem[
numbered=no,
style=colorquestion,
name=Question
]{question}

\title{\course \; \assignmentName}
\author{\textbf{\assignmentAuthor} \\ \Small{USC ID - \USCID}}
\date{\assignmentDate}

% ----------------------------

% The "stuff" above here is called the preamble of the document.  It sets the margins and loads special packages.  Probably the only reason you would need to edit something above here would be to add a package to do something very specific... but probably everything you need is loaded already

% -----------------------------

\begin{document}
\thispagestyle{plain}
\maketitle
\hrule
\bigskip

% -------------------------
% Question 1
% -------------------------

\begin{proof}[\color{red}{\textbf{Problem 1}: Gaussian Mixture Model and EM}]

\hfill

\begin{enumerate}[label={\color{blue}{\textbf{1.\arabic*})}}]
    \item 
        Given Equation: 
        \begin{align*}
            \argmax_{\omega_k, \mu_k, \sum_k} \sum\limits_{n} &\sum\limits_{k} \gamma_{nk} \ln{\omega_k} + \sum\limits_{n} \sum\limits_{k} \gamma_{nk} \ln{\mathcal{N}(x_n | \mu_k, \textstyle\sum\nolimits_{k})}\\
            &\text{s.t. } \omega_k \geq 0\\
            &\sum\limits_{k=1}^{K}\omega_k = 1
        \end{align*}
        
        To find the MLE or the optimal solution, we need to find the optimal values for $\omega_k, \mu_k, \textstyle\sum_k $\\
        \textbf{Let's find the value of optimal $\omega_k$},\\
        To find $\omega_k^*$ we need to solve the below equation, 
        \begin{align*}
            \argmax_{\omega_k} \sum\limits_{n} &\sum\limits_{k} \gamma_{nk} \ln{\omega_k} \\
           &\text{s.t. } \omega_k \geq 0\\ &\sum\limits_{k=1}^{K}\omega_k = 1
        \end{align*}
        Using the Lagrangian to solve the above, we will define $\lambda$ and $\lambda_k$ as Lagrangian multipliers.\\
        Applying the stationary condition for each $\omega_k$. We will get,
        \begin{align*}
            \frac{\textstyle\sum_n\gamma_{nk}}{\omega_k} + \lambda + \lambda_k = 0\\
            \omega_k^* = -\frac{\textstyle\sum_n\gamma_{nk}}{\lambda + \lambda_k} \neq 0
        \end{align*}
        By applying complementary slackness, we will get, 
        \begin{align*}
            \lambda_k\omega_k^* = 0\\
            \lambda_k = 0
        \end{align*}
        Applying feasibility, we will get, 
        \begin{align*}
            \sum\limits_{k=1}^{K}\omega_k^* = -\sum\limits_{k=1}^{K}\frac{\textstyle\sum_n\gamma_{nk}}{\lambda} = 1\\
            \lambda = -\sum\limits_{k=1}^{K}\sum_n\gamma_{nk} = -\textstyle\sum_n 1 = -N
        \end{align*}
        Using the values of $\lambda, \lambda_k$ found above, we can rewrite $\omega_k^*$ as,
        \begin{align*}
            \omega_k^* &= -\frac{\textstyle\sum_n\gamma_{nk}}{\lambda + \lambda_k} \\
            \omega_k^* &= \frac{\textstyle\sum_n\gamma_{nk}}{N}
        \end{align*}
        
        To find the values of $\mu_k, \textstyle\sum_k$, we need to solve, 
        \begin{align*}
            &\argmax_{\mu_k, \sum_k} \sum\limits_{n}  \gamma_{nk} \ln{\mathcal{N}(x_n | \mu_k, \textstyle\sum\nolimits_{k})}\\
            &= \argmax_{\mu_k, \sum_k} \sum\limits_{n}  \gamma_{nk} \ln{\bigg[ \frac{1}{(\sqrt{2\pi}\sigma_k)^D}\exp{\bigg( - \frac{1}{2\sigma_k^2}||x_n - \mu_k||^2 \bigg)} \bigg]}\\
            &= \argmax_{\mu_k, \sum_k} \sum\limits_{n}  \gamma_{nk} {\bigg( -D\ln{\sigma_k}  - \frac{||x_n - \mu_k||^2}{2\sigma_k^2} \bigg)} \tag*{(where D is length of $x_n$)}\\
        \end{align*}
        \textbf{Let's find the value of optimal $\mu_k$}, \\
        To find $\mu_k$, we will set the derivative with respect to $\mu_k$ to 0, we get
        \begin{align*}
            &\frac{1}{\sigma_k}\sum_n{\gamma_{nk}(x_n- \mu_k)} = 0\\
            &\mu_k^* = \frac{\textstyle
            \sum\nolimits_n{\gamma_{nk}x_n}}{\sum\nolimits_n{\gamma_{nk}}}
        \end{align*}
       \textbf{Let's find the value of optimal $\sigma_k$}, \\
        To find $\sigma_k$, we will set the derivative with respect to $\sigma_k$ to 0, we get
        \begin{align*}
            &\sum_n{\gamma_{nk}}\bigg(-\frac{D}{\sigma_k} + \frac{||x_n - \mu_k||^2}{\sigma_k^3}\bigg) = 0\\
            &(\sigma_k^*)^2 = \frac{\sum\nolimits_n\gamma_{nk}||x_n - \mu_k^*||^2}{D\sum\nolimits_n\gamma_{nk}}
        \end{align*}
        
        $\therefore$ \textbf{The optimal values are}, 
        \begin{align*}
            \omega_k^* &= \frac{\textstyle\sum_n\gamma_{nk}}{N}\\
            \mu_k^* &= \frac{\textstyle
            \sum\nolimits_n{\gamma_{nk}x_n}}{\sum\nolimits_n{\gamma_{nk}}}\\
            (\sigma_k^*)^2 &= \frac{\sum\nolimits_n\gamma_{nk}||x_n - \mu_k^*||^2}{D\sum\nolimits_n\gamma_{nk}}
        \end{align*}
    \item 
        To reduce GMM to K-means, we need to set the parameters as below\\
        Set weights of GMM as average or $\omega_k = \frac{1}{K}$ \\
        Set standard deviation, $\sigma_k = \sigma \to $ 0
        \begin{align*}
            p(x_n, z_n = k) &= p(z_n = k) p(x_n | z_n = k)\\
            &\propto \;\;p(x_n | z_n = k){\tag*{(ignoring constant terms)}}\\
            &\propto \;\; \exp{\bigg(  -\frac{1}{2\sigma_k^2} || x_n - \mu_k ||^2 \bigg)}{\tag*{(Gaussian distribution)}}
        \end{align*}
        To calculate the posterior probability $p(z_n = k | x_n)$, we can use the conditional probability, and set $\sigma_k = \sigma \to 0$
        \begin{align*}
            p(z_n = k | x_n) &= \frac{p(x_n, z_n = k)}{p(x_n)}\\
            p(z_n = k | x_n) &= \lim_{\sigma \to 0}\frac{\exp{\big( -\frac{1}{2\sigma_k^2} || x_n - \mu_k ||^2 \big)}}{\sum\nolimits_j\exp{\big(  -\frac{1}{2\sigma_j^2} || x_n - \mu_j ||^2 \big)}}\\
            p(z_n = k | x_n) &\to \begin{cases} 
              1 & \text{if $k = \argmin_j ||x_n - \mu_j||^2$} \\
              0 & \text{otherwise}
           \end{cases}
        \end{align*}
        $\therefore$ The above expression gives a hard assignment of either 1 or 0 to a particular cluster $z_n = k$ given $x_n$. Thus, we have reduced the GMM to K-means clustering algorithm.
\end{enumerate}
\end{proof}



% -------------------------
% Question 2
% -------------------------
\hrule
\bigskip

\begin{proof}[\color{red}{\textbf{Problem 2}: Hidden Markov Model}]

\hfill

\begin{enumerate}[label={\color{blue}{\textbf{2.\arabic*})}}]
    \item 
        We know that $\alpha_s(T) = P(X_{T}=s, O_{1:T}=o_{1:T})$. \\
        Derivation,
        \begin{align*}
        P(X_{T+1}=s | O_{1:T}=o_{1:T}) &=\frac{P(X_{T+1}=s , O_{1:T}=o_{1:T})}{P(O_{1:T}=o_{1:T})} \tag{Conditional Probability}\\
        &=\frac{\sum\nolimits_{s'}P(X_{T+1}=s,X_{T}=s', O_{1:T}=o_{1:T})}{P(O_{1:T}=o_{1:T})} \tag{Marginalisation}\\
        &=\frac{\sum\nolimits_{s'}P(X_{T+1}=s,X_{T}=s', O_{1:T}=o_{1:T})}{\sum\nolimits_{s''}P(X_T = s'', O_{1:T}=o_{1:T})} \tag{Marginalisation}\\
        &=\frac{\sum\nolimits_{s'}P(X_{T+1}=s|X_{T}=s', O_{1:T}=o_{1:T})P(X_{T}=s', O_{1:T}=o_{1:T})}{\sum\nolimits_{s''}P(X_T = s'', O_{1:T}=o_{1:T})} \tag{Conditional Probability}\\
        &=\frac{\sum\nolimits_{s'}P(X_{T+1}=s|X_{T}=s')P(X_{T}=s', O_{1:T}=o_{1:T})}{\sum\nolimits_{s''}P(X_T = s'', O_{1:T}=o_{1:T})} \tag{Markov Property}\\
        &=\frac{\sum\nolimits_{s'}a_{s',s}\alpha_{s'}(T)}{\sum\nolimits_{s''}\alpha_{s''}(T)} \tag{Definition for $\alpha_s(T)$}\\
    \end{align*}
        
        
    \item 
        To reduce HMM to GMM, we need to set the HMM model parameters in form of GMM weights and Gaussian distribution as below,
        
        \begin{itemize}
         \item Initial Distribution, $P(X_1=S)$
         \begin{align*}
             P(X_1=S) = \omega_s
         \end{align*}
         \item Transition Distribution, $P(X_{t+1}=s' | X_t=s)$
         \begin{align*}
             P(X_{t+1}=s'|X_t=s) 
             &=P(X_{t+1}
             = s')\\ 
             & = \omega_{s'}
         \end{align*}
         \item Emission Distribution, $P(O_{t}=o | X_t=s)$
         \begin{align*}
             P(O_{t}=o | X_t=s)  
             &=\mathcal{N}(o|\mu_s,\sigma^2_{s}I)
         \end{align*}
     \end{itemize}
     The above representation means that each state in HMM is a cluster in GMM.\\
     Applying factorization on $P(X_2=s| O_1=o_1,O_2=o_2)$,
     \begin{align*}
         P(X_2=s| O_1=o_1,O_2=o_2) & = \sum\nolimits_{s'}P(X_1=s', X_2=s| O_1=o_1,O_2=o_2) \tag{HMM Factorisation}
     \end{align*}
     Also, we can write
     \begin{align*}
         &P(X_1=s', X_2=s| O_1=o_1,O_2=o_2)  =\frac{P(X_1=s', X_2=s, O_1=o_1,O_2=o_2)}{\sum\nolimits_{s'}\sum\nolimits_{s}P(X_1=s', X_2=s,O_1=o_1,O_2=o_2)}\\
         &P(X_1=s', X_2=s, O_1=o_1,O_2=o_2)\\ 
          &= P(X_1=s')P(X_2=s| X_1=s')P(O_1=o_1|X_1=s')P(O_2=o_2|X_2=s)
     \end{align*}
     Let's replace the derived terms with the model parameters.
     \begin{align*}
         P(X_1=s', X_2=s, O_1=o_1,O_2=o_2) &=\omega_{s'}\omega_{s}\mathcal{N}(o_1 | \mu_{s'},\sigma^2_{s'}I)\mathcal{N}(o_2 | \mu_{s},\sigma^2_{s}I)\\
         &=\omega_{s'}\mathcal{N}(o_1 | \mu_{s'},\sigma^2_{s'}I)\omega_{s}\mathcal{N}(o_2 | \mu_{s},\sigma^2_{s}I)\\
         &=P(X_1=s',O =o_1)P(X_2=s,O =o_2)
     \end{align*}
     Hence, HMM in this case is non-sequential/non-temporal in nature. \\
     Rewriting, $P(X_2=s| O_1=o_1,O_2=o_2)$:
     \begin{align*}
         P(X_2=s| O_1=o_1,O_2=o_2) &= \frac{\sum\nolimits_{s'}\omega_{s'}\omega_{s}\mathcal{N}(o_1 | \mu_{s'},\sigma^2_{s'}I)\mathcal{N}(o_2 | \mu_{s},\sigma^2_{s}I)}{\sum\nolimits_{s'}\sum\nolimits_{s}\omega_{s'}\omega_{s}\mathcal{N}(o_1 | \mu_{s'},\sigma^2_{s'}I)\mathcal{N}(o_2 | \mu_{s},\sigma^2_{s}I)}\\
         &= \frac{(\sum\nolimits_{s'}\omega_{s'}\mathcal{N}(o_1 | \mu_{s'},\sigma^2_{s'}I))(\omega_{s}\mathcal{N}(o_2 | \mu_{s},\sigma^2_{s}I))}{(\sum\nolimits_{s'}\omega_{s'}\mathcal{N}(o_1 | \mu_{s'},\sigma^2_{s'}I))(\sum\nolimits_{s}\omega_{s}\mathcal{N}(o_2 | \mu_{s},\sigma^2_{s}I))}\\
         &= \frac{(\omega_{s}\mathcal{N}(o_2 | \mu_{s},\sigma^2_{s}I))}{(\sum\nolimits_{s}\omega_{s}\mathcal{N}(o_2 | \mu_{s},\sigma^2_{s}I))}
     \end{align*}
     Hence, we have reduced HMM to GMM. Also, we can see that the posterior probability is independent of $O_1$ and $X_1$ and it is similar to that of GMM.
    
\end{enumerate}
\end{proof}
% -------------------------
% Question 3
% -------------------------
\hrule
\bigskip

\begin{proof}[\color{red}{\textbf{Problem 3}: Viterbi Algorithm}]
\hfill
\begin{enumerate}[label={\color{blue}{\textbf{3.\arabic*})}}]
    \item 
        To calculate $P(O_{1:4}=o_{1:4};\theta)$, we need to compute $\sum_s\alpha_s(T)$. 
        \begin{align*}
            \alpha_1(1) &= \pi_1.b_{1A} = 0.6 * 0.4 = 0.24 \\
            \alpha_2(1) &= \pi_2.b_{2A} = 0.4 * 0.2 = 0.08\\
            \alpha_1(2) &= \sum_s\alpha_s(1).a_{s1}.b_{1G}\\
            &= \alpha_1(1).a_{11}.b_{1G} + \alpha_2(1).a_{21}.b_{1G}\\
            &= 0.24*0.7*0.4 + 0.08*0.2*0.4\\
            &= 0.0736\\
            \alpha_2(2) &= \sum_s\alpha_s(1).a_{s2}.b_{2G}\\
            &= \alpha_1(1).a_{12}.b_{2G} + \alpha_2(1).a_{22}.b_{2G}\\
            &= 0.24*0.3*0.2 + 0.08*0.8*0.2\\
            &= 0.0272\\
            \alpha_1(3) &= \sum_s\alpha_s(2).a_{s1}.b_{1C}\\
            &= \alpha_1(2).a_{11}.b_{1C} + \alpha_2(2).a_{21}.b_{1C}\\
            &= 0.0736*0.7*0.1 + 0.0272*0.2*0.1\\
            &= 0.005696\\
            \alpha_2(3) &= \sum_s\alpha_s(2).a_{s2}.b_{2C}\\
            &= \alpha_1(2).a_{12}.b_{2C} + \alpha_2(2).a_{22}.b_{2C}\\
            &= 0.0736*0.3*0.3 + 0.0272*0.8*0.3\\
            &= 0.013152\\
            \alpha_1(4) &= \sum_s\alpha_s(3).a_{s1}.b_{1T}\\
            &= \alpha_1(3).a_{11}.b_{1T} + \alpha_2(3).a_{21}.b_{1T}\\
            &= 0.005696*0.7*0.1 + 0.013152*0.2*0.1\\
            &= 0.00066176\\
            \alpha_2(4) &= \sum_s\alpha_s(3).a_{s2}.b_{2T}\\
            &= \alpha_1(3).a_{12}.b_{2T} + \alpha_2(3).a_{22}.b_{2T}\\
            &= 0.005696*0.3*0.3 + 0.013152*0.8*0.3\\
            &= 0.00366912\\
        \end{align*}
        There, the final value of $P(O_{1:4}=o_{1:4};\theta)$ is,
        \begin{align*}
            P(O_{1:4}=o_{1:4};\theta) &= \sum_s\alpha_s(T) \\
            &= \sum_s\alpha_s(4) \\
            &= \alpha_1(4) + \alpha_2(4) \\
            &= 0.00066176 + 0.00366912\\
            &= \bm{0.00433088}
        \end{align*}
        
        
    \item 
        To calculate $s^*_{1:4} = [s^*_{1},s^*_{2},s^*_{3},s^*_{4}]$, we will use Viterbi Algorithm.
        
        \begin{align*}
            \delta_{1}(1) &= \pi_1.b_{1A} = 0.6*0.4 = 0.24\\
            \delta_{2}(1) &= \pi_2.b_{2A} = 0.4*0.2 = 0.08\\
            \delta_{1}(2) &= b_{1G}\max\limits_sa_{s1}\delta_{s}(1)\\
            &= b_{1G}\max\limits_s \{ a_{11}\delta_{1}(1), a_{21}\delta_{2}(1) \}\\
            &= 0.4*\max\limits_s \{ 0.7*0.24, 0.2*0.08 \}\\
            &= 0.0672\\
            \Delta_{1}(2) &= \argmax\limits_sa_{s1}\delta_{s}(1)\\
            \Delta_{1}(2) &= 1\\
            \delta_{2}(2) &= b_{2G}\max\limits_sa_{s2}\delta_{s}(1)\\
            &= b_{2G}\max\limits_s \{ a_{12}\delta_{1}(1), a_{22}\delta_{2}(1) \}\\
            &= 0.2*\max\limits_s \{ 0.3*0.24, 0.8*0.08 \}\\
            &= 0.0144\\
            \Delta_{2}(2) &= \argmax\limits_sa_{s2}\delta_{s}(1)\\
            \Delta_{2}(2) &= 1\\
            \delta_{1}(3) &= b_{1C}\max\limits_sa_{s1}\delta_{s}(2)\\
            &= b_{1C}\max\limits_s \{ a_{11}\delta_{1}(2), a_{21}\delta_{2}(2) \}\\
            &= 0.1*\max\limits_s \{ 0.7*0.0672, 0.2*0.0144 \}\\
            &= 0.004704\\
            \Delta_{1}(3) &= \argmax\limits_sa_{s1}\delta_{s}(2)\\
            \Delta_{1}(3) &= 1\\
            \delta_{2}(3) &= b_{2C}\max\limits_sa_{s2}\delta_{s}(2)\\
            &= b_{2C}\max\limits_s \{ a_{12}\delta_{1}(2), a_{22}\delta_{2}(2) \}\\
            &= 0.3*\max\limits_s \{ 0.3*0.0672, 0.8*0.0144 \}\\
            &= 0.006048\\
            \Delta_{2}(3) &= \argmax\limits_sa_{s2}\delta_{s}(2)\\
            \Delta_{2}(3) &= 1\\
        \end{align*}
        \begin{align*}
            \delta_{1}(4) &= b_{1T}\max\limits_sa_{s1}\delta_{s}(3)\\
            &= b_{1T}\max\limits_s \{ a_{11}\delta_{1}(3), a_{21}\delta_{2}(3) \}\\
            &= 0.1*\max\limits_s \{ 0.7*0.004704, 0.2*0.006048 \}\\
            &= 0.00032928\\
            \Delta_{1}(4) &= \argmax\limits_sa_{s1}\delta_{s}(3)\\
            \Delta_{1}(4) &= 1\\
            \delta_{2}(4) &= b_{2T}\max\limits_sa_{s2}\delta_{s}(3)\\
            &= b_{2T}\max\limits_s \{ a_{12}\delta_{1}(3), a_{22}\delta_{2}(3) \}\\
            &= 0.3*\max\limits_s \{ 0.3*0.004704, 0.8*0.006048 \}\\
            &= 0.00145152\\
            \Delta_{2}(4) &= \argmax\limits_sa_{s2}\delta_{s}(3)\\
            \Delta_{2}(4) &= 2
        \end{align*}
        
        Hence, as $\delta_{2}(4)$ is larger than $\delta_{2}(4)$. We will start with $\Delta_{2}(4) = 2$. Then backtracking from there, we will decode the states as [1, 1, 2, 2]. \\
        $\therefore$ The final hidden states decoded from Viterbi algorithm is \textbf{[1, 1, 2, 2]}.
        
    \item 
        To find the most likely $O_5$, first we will calculate,
        \begin{align*}
            P(O_5=o_5 | O_{1:4}=o_{1:4}) &= \frac{P(O_5=o_5, O_{1:4}=o_{1:4})}{P(O_{1:4}=o_{1:4})}\\
            &= \frac{P(O_{1:5}=o_{1:5})}{P(O_{1:4}=o_{1:4})}\\
            &=\frac{\sum\nolimits_sP(O_{1:5}=o_{1:5}, X_5 = s)}{P(O_{1:4}=o_{1:4})}{\tag*{(Marginalization)}}\\
            &=\frac{\sum\nolimits_s\alpha_s(5)}{P(O_{1:4}=o_{1:4})}\\
            &=\frac{\alpha_1(5) + \alpha_2(5)}{P(O_{1:4}=o_{1:4})}\\
            &=\frac{(\sum\nolimits_s\alpha_s(4).a_{s1}.b_{1O_5}) + (\sum\nolimits_s\alpha_s(4).a_{s2}.b_{2O_5})}{P(O_{1:4}=o_{1:4})}\\
            &=\frac{(\alpha_1(4).a_{11}.b_{1O_5} + \alpha_2(4).a_{21}.b_{1O_5}) + (\alpha_1(4).a_{12}.b_{2O_5} + \alpha_2(4).a_{22}.b_{2O_5})}{P(O_{1:4}=o_{1:4})}\\
        \end{align*}
        Now, taking the argmax over $o_5$.
    \begin{align*}
        &\argmax_{o_5 \in \{A, G, C, T\}}P(O_5=o_5 | O_{1:4} = o_{1:4})\\
            &= \argmax\{(\alpha_1(4).a_{11}.b_{1O_5} + \alpha_2(4).a_{21}.b_{1O_5}) + (\alpha_1(4).a_{12}.b_{2O_5} + \alpha_2(4).a_{22}.b_{2O_5})\}\\
    \end{align*}
    Calculating for each A, G, C, T,
    \begin{align*}
        &P(O_5=A | O_{1:4}=o_{1:4})\\
        &\propto (\alpha_1(4).a_{11}.b_{1A} + \alpha_2(4).a_{21}.b_{1A}) + (\alpha_1(4).a_{12}.b_{2A} + \alpha_2(4).a_{22}.b_{2A})\\
        &= 0.00066176*0.7*0.4 + 0.00366912*0.2*0.4 + 0.00066176*0.3*0.2 + 0.00366912*0.8*0.2\\
        &= 0.0011055872
    \end{align*}
    \begin{align*}
        &P(O_5=G | O_{1:4}=o_{1:4})\\
        &\propto (\alpha_1(4).a_{11}.b_{1G} + \alpha_2(4).a_{21}.b_{1G}) + (\alpha_1(4).a_{12}.b_{2G} + \alpha_2(4).a_{22}.b_{2G})\\
        &= 0.00066176*0.7*0.4 + 0.00366912*0.2*0.4 + 0.00066176*0.3*0.2 + 0.00366912*0.8*0.2\\
        &= 0.0011055872
    \end{align*}
    \begin{align*}
        &P(O_5=C | O_{1:4}=o_{1:4})\\
        &\propto (\alpha_1(4).a_{11}.b_{1C} + \alpha_2(4).a_{21}.b_{1C}) + (\alpha_1(4).a_{12}.b_{2C} + \alpha_2(4).a_{22}.b_{2C})\\
        &= 0.00066176*0.7*0.1 + 0.00366912*0.2*0.1 + 0.00066176*0.3*0.3 + 0.00366912*0.8*0.3\\
        &= 0.0010598528
    \end{align*}
    \begin{align*}
        &P(O_5=T | O_{1:4}=o_{1:4})\\
        &\propto (\alpha_1(4).a_{11}.b_{1T} + \alpha_2(4).a_{21}.b_{1T}) + (\alpha_1(4).a_{12}.b_{2T} + \alpha_2(4).a_{22}.b_{2T})\\
        &= 0.00066176*0.7*0.1 + 0.00366912*0.2*0.1 + 0.00066176*0.3*0.3 + 0.00366912*0.8*0.3\\
        &= 0.0010598528
    \end{align*}
    Now to get the most likely $O_5$, taking argmax over $O_5$ of the above expression.
    \begin{align*}
            &\argmax_{o_5 \in \{A, G, C, T\}}P(O_5=o_5 | o_{1:4})\\
            &\argmax\{P(O_5=A | o_{1:4}), P(O_5=G | o_{1:4}), P(O_5=C | o_{1:4}), P(O_5=T | o_{1:4})\}\\
            &= A \text{ or } G \tag*{(Using the above values)}
    \end{align*}
    $\therefore$ Any of the \textbf{A or G} is the most likely next prediction.
\end{enumerate}

\end{proof}
\hrule
\bigskip

\end{document}
